{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from math import exp\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as utils\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pytorch_ssim\n",
    "from torchvision.transforms import Compose, RandomCrop, ToTensor, ToPILImage, CenterCrop, Resize\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsampleBLock(nn.Module):\n",
    "    def __init__(self, in_channels, up_scale):\n",
    "        super(UpsampleBLock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels * up_scale ** 2, kernel_size=3, padding=1)\n",
    "        self.pixel_shuffle = nn.PixelShuffle(up_scale)\n",
    "        self.prelu = nn.PReLU()\n",
    "    def forward(self, x):\n",
    "        return self.prelu(self.pixel_shuffle(self.conv(x)))\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels=64):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.prelu = nn.PReLU()\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        return self.bn2(self.conv2(self.prelu(self.bn1(self.conv1(x))))) + x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, scale_factor):\n",
    "        upsample_block_num = int(math.log(scale_factor, 2))\n",
    "\n",
    "        super(Generator, self).__init__()\n",
    "        self.b1 = nn.Sequential(nn.Conv2d(3, 64, kernel_size=9, padding=4), nn.PReLU())\n",
    "        self.b2 = nn.Sequential(*[ResidualBlock(64) for _ in range(16)])\n",
    "        self.b3 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64))\n",
    "        self.b4 = nn.Sequential(*[UpsampleBLock(64, 2) for _ in range(upsample_block_num)])\n",
    "        self.tail = nn.Conv2d(64, 3, kernel_size=9, padding=4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        start = self.b1(x)\n",
    "        end = self.b4(self.b3(self.b2(start)) + start)\n",
    "        return self.tail(end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        ch = [64, 64, 128, 128, 256, 256, 512, 512]\n",
    "        body = []\n",
    "        for i in range(1, len(ch)):\n",
    "            body.extend([nn.Conv2d(ch[i-1], ch[i], kernel_size=3, stride=2, padding=1),\n",
    "                         nn.BatchNorm2d(ch[i]),\n",
    "                         nn.LeakyReLU(0.2)])\n",
    "        self.start = nn.Sequential(nn.Conv2d(3, 64, kernel_size=3, padding=1), nn.LeakyReLU(0.2))    \n",
    "        self.body = nn.Sequential(*body)\n",
    "        self.tail = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Conv2d(512, 1024, kernel_size=1),\n",
    "                                  nn.LeakyReLU(0.2), nn.Conv2d(1024, 1, kernel_size=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.tail(self.body(self.start(x))).view(x.size(0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GLoss, self).__init__()\n",
    "        vgg = models.vgg16(pretrained=True)\n",
    "        vgg_part = nn.Sequential(*list(vgg.features)[:31]).eval()\n",
    "        for param in vgg_part.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.vgg = vgg_part\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, out_labels, out_images, target_images):\n",
    "        adversarial_loss = torch.mean(1 - out_labels)\n",
    "        perception_loss = self.mse_loss(self.vgg(out_images), self.vgg(target_images))\n",
    "        image_loss = self.mse_loss(out_images, target_images)\n",
    "        return image_loss + 0.001 * adversarial_loss + 0.006 * perception_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_transform():\n",
    "    return Compose([\n",
    "        ToPILImage(),\n",
    "        Resize(400),\n",
    "        CenterCrop(400),\n",
    "        ToTensor()\n",
    "    ])\n",
    "\n",
    "class TrainDatasetFromFolder(Dataset):\n",
    "    def __init__(self, dataset_dir, crop_size, upscale_factor):\n",
    "        super(TrainDatasetFromFolder, self).__init__()\n",
    "        self.image_filenames = [join(dataset_dir, i) for i in listdir(dataset_dir) if i.endswith('.jpg')][:90000]\n",
    "        self.upsc = upscale_factor\n",
    "        self.crop_size = crop_size - (crop_size % upscale_factor)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.image_filenames[index]).convert(mode='RGB')\n",
    "        hr_image = RandomCrop(self.crop_size)(img)\n",
    "        lr_image = Resize(self.crop_size // self.upsc, interpolation=Image.BICUBIC)(hr_image)\n",
    "        try:\n",
    "            return ToTensor()(lr_image), ToTensor()(hr_image)\n",
    "        except:\n",
    "            print(self.image_filenames[index])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "\n",
    "class ValDatasetFromFolder(Dataset):\n",
    "    def __init__(self, dataset_dir, crop_size, upscale_factor):\n",
    "        super(ValDatasetFromFolder, self).__init__()\n",
    "        self.upscale_factor = upscale_factor\n",
    "        self.crop_size = crop_size - (crop_size % upscale_factor)\n",
    "        self.image_filenames = [join(dataset_dir, i) for i in listdir(dataset_dir) if i.endswith('.jpg')][90000:]\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        hr_image = Image.open(self.image_filenames[index]).convert(mode='RGB')\n",
    "        w, h = hr_image.size\n",
    "        crop_size = min(w, h) - (min(w, h) % self.upscale_factor)\n",
    "        hr_image = CenterCrop(self.crop_size)(hr_image)\n",
    "        lr_image = Resize(self.crop_size // self.upscale_factor, interpolation=Image.BICUBIC)(hr_image)\n",
    "        hr_restore_img = Resize(self.crop_size, interpolation=Image.BICUBIC)(lr_image)\n",
    "        return ToTensor()(lr_image), ToTensor()(hr_restore_img), ToTensor()(hr_image)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size // 2) ** 2 / float(2 * sigma ** 2)) for x in range(window_size)])\n",
    "    return gauss / gauss.sum()\n",
    "\n",
    "\n",
    "def create_window(window_size, channel):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
    "    return window\n",
    "\n",
    "\n",
    "def _ssim(img1, img2, window, window_size, channel, size_average=True):\n",
    "    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2\n",
    "\n",
    "    C1 = 0.01 ** 2\n",
    "    C2 = 0.03 ** 2\n",
    "\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    if size_average:\n",
    "        return ssim_map.mean()\n",
    "    else:\n",
    "        return ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "\n",
    "class SSIM(torch.nn.Module):\n",
    "    def __init__(self, window_size=11, size_average=True):\n",
    "        super(SSIM, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.size_average = size_average\n",
    "        self.channel = 1\n",
    "        self.window = create_window(window_size, self.channel)\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        (_, channel, _, _) = img1.size()\n",
    "\n",
    "        if channel == self.channel and self.window.data.type() == img1.data.type():\n",
    "            window = self.window\n",
    "        else:\n",
    "            window = create_window(self.window_size, channel)\n",
    "\n",
    "            if img1.is_cuda:\n",
    "                window = window.cuda(img1.get_device())\n",
    "            window = window.type_as(img1)\n",
    "\n",
    "            self.window = window\n",
    "            self.channel = channel\n",
    "\n",
    "        return _ssim(img1, img2, window, self.window_size, channel, self.size_average)\n",
    "\n",
    "\n",
    "def ssim(img1, img2, window_size=11, size_average=True):\n",
    "    (_, channel, _, _) = img1.size()\n",
    "    window = create_window(window_size, channel)\n",
    "\n",
    "    if img1.is_cuda:\n",
    "        window = window.cuda(img1.get_device())\n",
    "    window = window.type_as(img1)\n",
    "\n",
    "    return _ssim(img1, img2, window, window_size, channel, size_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1/40] Loss_D: 1.0000 Loss_G: 0.0087 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:38<00:00,  2.43it/s]\n",
      "PSNR: 20.8755 dB SSIM: 0.6449: 100%|██████████| 283/283 [00:20<00:00, 14.14it/s]\n",
      "[2/40] Loss_D: 1.0000 Loss_G: 0.0087 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:40<00:00,  2.42it/s]\n",
      "PSNR: 20.9046 dB SSIM: 0.6468: 100%|██████████| 283/283 [00:20<00:00, 13.98it/s]\n",
      "[3/40] Loss_D: 1.0000 Loss_G: 0.0087 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:40<00:00,  2.43it/s]\n",
      "PSNR: 20.8827 dB SSIM: 0.6449: 100%|██████████| 283/283 [00:19<00:00, 14.20it/s]\n",
      "[4/40] Loss_D: 1.0000 Loss_G: 0.0087 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:40<00:00,  2.43it/s]\n",
      "PSNR: 20.8643 dB SSIM: 0.6447: 100%|██████████| 283/283 [00:19<00:00, 14.25it/s]\n",
      "[5/40] Loss_D: 1.0000 Loss_G: 0.0086 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:40<00:00,  2.43it/s]\n",
      "PSNR: 20.9151 dB SSIM: 0.6475: 100%|██████████| 283/283 [00:19<00:00, 14.27it/s]\n",
      "[6/40] Loss_D: 1.0000 Loss_G: 0.0086 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:39<00:00,  2.43it/s]\n",
      "PSNR: 20.8837 dB SSIM: 0.6448: 100%|██████████| 283/283 [00:19<00:00, 14.17it/s]\n",
      "[7/40] Loss_D: 1.0000 Loss_G: 0.0087 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:39<00:00,  2.43it/s]\n",
      "PSNR: 20.9190 dB SSIM: 0.6467: 100%|██████████| 283/283 [00:20<00:00, 14.09it/s]\n",
      "[8/40] Loss_D: 1.0000 Loss_G: 0.0087 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:39<00:00,  2.43it/s]\n",
      "PSNR: 20.9295 dB SSIM: 0.6478: 100%|██████████| 283/283 [00:20<00:00, 14.10it/s]\n",
      "[9/40] Loss_D: 1.0000 Loss_G: 0.0087 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:39<00:00,  2.43it/s]\n",
      "PSNR: 20.9112 dB SSIM: 0.6462: 100%|██████████| 283/283 [00:20<00:00, 13.97it/s]\n",
      "[10/40] Loss_D: 1.0000 Loss_G: 0.0087 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:39<00:00,  2.43it/s]\n",
      "PSNR: 20.8696 dB SSIM: 0.6442: 100%|██████████| 283/283 [00:19<00:00, 14.23it/s]\n",
      "[11/40] Loss_D: 1.0000 Loss_G: 0.0087 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:38<00:00,  2.43it/s]\n",
      "PSNR: 20.9054 dB SSIM: 0.6456: 100%|██████████| 283/283 [00:19<00:00, 14.22it/s]\n",
      "[12/40] Loss_D: 1.0000 Loss_G: 0.0087 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:39<00:00,  2.43it/s]\n",
      "PSNR: 20.9554 dB SSIM: 0.6488: 100%|██████████| 283/283 [00:19<00:00, 14.29it/s]\n",
      "[13/40] Loss_D: 1.0000 Loss_G: 0.0086 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:38<00:00,  2.43it/s]\n",
      "PSNR: 21.0314 dB SSIM: 0.6529: 100%|██████████| 283/283 [00:20<00:00, 14.07it/s]\n",
      "[14/40] Loss_D: 1.0000 Loss_G: 0.0087 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:38<00:00,  2.43it/s]\n",
      "PSNR: 20.8968 dB SSIM: 0.6455: 100%|██████████| 283/283 [00:19<00:00, 14.30it/s]\n",
      "[15/40] Loss_D: 1.0000 Loss_G: 0.0087 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:37<00:00,  2.44it/s]\n",
      "PSNR: 20.9260 dB SSIM: 0.6473: 100%|██████████| 283/283 [00:19<00:00, 14.36it/s]\n",
      "[16/40] Loss_D: 1.0000 Loss_G: 0.0087 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:38<00:00,  2.43it/s]\n",
      "PSNR: 20.9070 dB SSIM: 0.6470: 100%|██████████| 283/283 [00:19<00:00, 14.20it/s]\n",
      "[17/40] Loss_D: 1.0000 Loss_G: 0.0086 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:38<00:00,  2.43it/s]\n",
      "PSNR: 20.8386 dB SSIM: 0.6419: 100%|██████████| 283/283 [00:19<00:00, 14.21it/s]\n",
      "[18/40] Loss_D: 1.0000 Loss_G: 0.0086 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:38<00:00,  2.43it/s]\n",
      "PSNR: 20.8901 dB SSIM: 0.6441: 100%|██████████| 283/283 [00:19<00:00, 14.22it/s]\n",
      "[19/40] Loss_D: 1.0000 Loss_G: 0.0087 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:38<00:00,  2.43it/s]\n",
      "PSNR: 20.8602 dB SSIM: 0.6443: 100%|██████████| 283/283 [00:19<00:00, 14.15it/s]\n",
      "[20/40] Loss_D: 1.0000 Loss_G: 0.0087 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:38<00:00,  2.43it/s]\n",
      "PSNR: 20.8252 dB SSIM: 0.6417: 100%|██████████| 283/283 [00:20<00:00, 13.81it/s]\n",
      "[21/40] Loss_D: 1.0000 Loss_G: 0.0087 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:38<00:00,  2.43it/s]\n",
      "PSNR: 20.8837 dB SSIM: 0.6445: 100%|██████████| 283/283 [00:19<00:00, 14.24it/s]\n",
      "[22/40] Loss_D: 1.0000 Loss_G: 0.0087 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:37<00:00,  2.43it/s]\n",
      "PSNR: 20.9333 dB SSIM: 0.6478: 100%|██████████| 283/283 [00:19<00:00, 14.23it/s]\n",
      "[23/40] Loss_D: 1.0000 Loss_G: 0.0087 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:38<00:00,  2.43it/s]\n",
      "PSNR: 20.8948 dB SSIM: 0.6456: 100%|██████████| 283/283 [00:19<00:00, 14.15it/s]\n",
      "[24/40] Loss_D: 1.0000 Loss_G: 0.0087 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:38<00:00,  2.43it/s]\n",
      "PSNR: 20.9506 dB SSIM: 0.6478: 100%|██████████| 283/283 [00:19<00:00, 14.20it/s]\n",
      "[25/40] Loss_D: 1.0000 Loss_G: 0.0087 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:38<00:00,  2.43it/s]\n",
      "PSNR: 20.9459 dB SSIM: 0.6485: 100%|██████████| 283/283 [00:19<00:00, 14.19it/s]\n",
      "[26/40] Loss_D: 1.0000 Loss_G: 0.0087 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:38<00:00,  2.43it/s]\n",
      "PSNR: 20.8654 dB SSIM: 0.6429: 100%|██████████| 283/283 [00:20<00:00, 14.01it/s]\n",
      "[27/40] Loss_D: 1.0000 Loss_G: 0.0087 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:37<00:00,  2.43it/s]\n",
      "PSNR: 20.9121 dB SSIM: 0.6456: 100%|██████████| 283/283 [00:20<00:00, 13.92it/s]\n",
      "[28/40] Loss_D: 1.0000 Loss_G: 0.0087 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:38<00:00,  2.43it/s]\n",
      "PSNR: 20.8726 dB SSIM: 0.6451: 100%|██████████| 283/283 [00:19<00:00, 14.23it/s]\n",
      "[29/40] Loss_D: 1.0000 Loss_G: 0.0087 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:38<00:00,  2.43it/s]\n",
      "PSNR: 20.8990 dB SSIM: 0.6462: 100%|██████████| 283/283 [00:20<00:00, 14.10it/s]\n",
      "[30/40] Loss_D: 1.0000 Loss_G: 0.0086 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:38<00:00,  2.43it/s]\n",
      "PSNR: 20.8915 dB SSIM: 0.6455: 100%|██████████| 283/283 [00:20<00:00, 14.14it/s]\n",
      "[31/40] Loss_D: 1.0000 Loss_G: 0.0087 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:37<00:00,  2.44it/s]\n",
      "PSNR: 20.8948 dB SSIM: 0.6455: 100%|██████████| 283/283 [00:19<00:00, 14.18it/s]\n",
      "[32/40] Loss_D: 1.0000 Loss_G: 0.0087 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:42<00:00,  2.41it/s]\n",
      "PSNR: 20.9268 dB SSIM: 0.6476: 100%|██████████| 283/283 [00:19<00:00, 14.30it/s]\n",
      "[33/40] Loss_D: 0.9970 Loss_G: 0.0090 D(x): 0.7134 D(G(z)): 0.7085: 100%|██████████| 1407/1407 [09:48<00:00,  2.39it/s]\n",
      "PSNR: 20.8798 dB SSIM: 0.6440: 100%|██████████| 283/283 [00:20<00:00, 14.11it/s]\n",
      "[34/40] Loss_D: 0.9944 Loss_G: 0.0091 D(x): 0.6016 D(G(z)): 0.5946: 100%|██████████| 1407/1407 [09:49<00:00,  2.39it/s]\n",
      "PSNR: 20.8657 dB SSIM: 0.6438: 100%|██████████| 283/283 [00:20<00:00, 14.14it/s]\n",
      "[35/40] Loss_D: 0.9956 Loss_G: 0.0091 D(x): 0.5639 D(G(z)): 0.5586: 100%|██████████| 1407/1407 [09:49<00:00,  2.39it/s]\n",
      "PSNR: 20.8691 dB SSIM: 0.6448: 100%|██████████| 283/283 [00:19<00:00, 14.18it/s]\n",
      "[36/40] Loss_D: 0.9957 Loss_G: 0.0090 D(x): 0.6971 D(G(z)): 0.6917: 100%|██████████| 1407/1407 [09:48<00:00,  2.39it/s]\n",
      "PSNR: 20.9629 dB SSIM: 0.6490: 100%|██████████| 283/283 [00:19<00:00, 14.25it/s]\n",
      "[37/40] Loss_D: 0.9918 Loss_G: 0.0090 D(x): 0.6786 D(G(z)): 0.6692: 100%|██████████| 1407/1407 [09:48<00:00,  2.39it/s]\n",
      "PSNR: 20.8736 dB SSIM: 0.6428: 100%|██████████| 283/283 [00:20<00:00, 14.14it/s]\n",
      "[38/40] Loss_D: 1.0001 Loss_G: 0.0087 D(x): 0.9995 D(G(z)): 0.9996: 100%|██████████| 1407/1407 [09:47<00:00,  2.39it/s]\n",
      "PSNR: 20.9193 dB SSIM: 0.6473: 100%|██████████| 283/283 [00:20<00:00, 14.14it/s]\n",
      "[39/40] Loss_D: 1.0000 Loss_G: 0.0087 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:47<00:00,  2.40it/s]\n",
      "PSNR: 20.8644 dB SSIM: 0.6446: 100%|██████████| 283/283 [00:19<00:00, 14.30it/s]\n",
      "[40/40] Loss_D: 1.0000 Loss_G: 0.0086 D(x): 1.0000 D(G(z)): 1.0000: 100%|██████████| 1407/1407 [09:45<00:00,  2.40it/s]\n",
      "PSNR: 20.9845 dB SSIM: 0.6493: 100%|██████████| 283/283 [00:20<00:00, 14.08it/s]\n"
     ]
    }
   ],
   "source": [
    "params = {'crop_size': 96, 'upscale_factor': 4, 'num_epochs':40, 'batch_size': 64}\n",
    "crop_size = params['crop_size']\n",
    "upscale = params['upscale_factor']\n",
    "num_epochs = params['num_epochs']\n",
    "batch_size = params['batch_size']\n",
    "\n",
    "train_set = TrainDatasetFromFolder('/mnt/storage-500g/datasets/VGDataset/VG_100K_vkz', crop_size=crop_size, upscale_factor=upscale)\n",
    "val_set = ValDatasetFromFolder('/mnt/storage-500g/datasets/VGDataset/VG_100K_vkz', crop_size=crop_size, upscale_factor=upscale)\n",
    "train_loader = DataLoader(dataset=train_set, num_workers=4, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_set, num_workers=4, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "G = Generator(upscale)\n",
    "D = Discriminator()\n",
    "G.load_state_dict(torch.load('epochs/G_best.pth'))\n",
    "D.load_state_dict(torch.load('epochs/D_best.pth'))\n",
    "G = G.to(device)\n",
    "D = D.to(device)\n",
    "GLoss = GLoss().to(device)\n",
    "\n",
    "\n",
    "optimizerG = optim.Adam(G.parameters(), lr=0.0002, betas=(0.9, 0.999))\n",
    "optimizerD = optim.Adam(D.parameters(), lr=0.0002, betas=(0.9, 0.999))\n",
    "schedulerD = torch.optim.lr_scheduler.StepLR(optimizerD, step_size=params['num_epochs'] // 2, gamma=0.1)\n",
    "schedulerG = torch.optim.lr_scheduler.StepLR(optimizerG, step_size=params['num_epochs'] // 2, gamma=0.1)\n",
    "\n",
    "results = {'d_loss': [], 'g_loss': [], 'd_score': [], 'g_score': [], 'psnr': [], 'ssim': []}\n",
    "\n",
    "n_iter = 0\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_bar = tqdm(train_loader)\n",
    "    epoch_res = {'batch_size': 0, 'd_loss': 0, 'g_loss': 0, 'd_score': 0, 'g_score': 0}\n",
    "\n",
    "    G.train()\n",
    "    D.train()\n",
    "    for n, (data, real_img) in enumerate(train_bar):\n",
    "        if data is None:\n",
    "            continue\n",
    "        epoch_res['batch_size'] += batch_size\n",
    "        data = data.to(device)\n",
    "        real_img = real_img.to(device)\n",
    "        fake_img = G(data)\n",
    "\n",
    "        D.zero_grad()\n",
    "        real_out = D(real_img).mean()\n",
    "        fake_out = D(fake_img).mean()\n",
    "        d_loss = 1 - real_out + fake_out\n",
    "        d_loss.backward(retain_graph=True)\n",
    "        optimizerD.step()\n",
    "        G.zero_grad()\n",
    "        g_loss = GLoss(Variable(fake_out.detach(), requires_grad=True),\n",
    "                       Variable(fake_img.detach(), requires_grad=True),\n",
    "                       Variable(real_img.detach(), requires_grad=True))\n",
    "        g_loss.backward()\n",
    "\n",
    "        fake_img = G(data)\n",
    "        fake_out = D(fake_img).mean()\n",
    "\n",
    "        optimizerG.step()\n",
    "\n",
    "        epoch_res['g_loss'] += g_loss.item()\n",
    "        epoch_res['d_loss'] += d_loss.item()\n",
    "        epoch_res['d_score'] += real_out.item()\n",
    "        epoch_res['g_score'] += fake_out.item()\n",
    "        n_iter += 1\n",
    "        if n % 1000 == 0:\n",
    "            mul = batch_size / epoch_res['batch_size']\n",
    "            train_bar.set_description(desc='[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f' % (\n",
    "                epoch, num_epochs, epoch_res['d_loss'] * mul, epoch_res['g_loss'] * mul,\n",
    "                epoch_res['d_score'] * mul, epoch_res['g_score'] * mul))\n",
    "\n",
    "    G.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_bar = tqdm(val_loader)\n",
    "        val_res = {'mse': 0, 'ssims': 0, 'psnr': 0, 'ssim': 0, 'batch_size': 0}\n",
    "        for n, (lr, hr_restore, hr) in enumerate(val_bar):\n",
    "            val_res['batch_size'] += batch_size\n",
    "            lr = lr.to(device)\n",
    "            hr = hr.to(device)\n",
    "            sr = G(lr)\n",
    "\n",
    "            batch_mse = ((sr - hr) ** 2).data.mean()\n",
    "            val_res['mse'] += batch_mse * batch_size\n",
    "            batch_ssim = pytorch_ssim.ssim(sr, hr).item()\n",
    "            val_res['ssims'] += batch_ssim * batch_size\n",
    "            val_res['psnr'] = 10 * math.log10((hr.max()**2) / (val_res['mse'] / val_res['batch_size']))\n",
    "            val_res['ssim'] = val_res['ssims'] / val_res['batch_size']\n",
    "            if n % 150 == 0:\n",
    "                val_bar.set_description(desc='PSNR: %.4f dB SSIM: %.4f' % (val_res['psnr'], val_res['ssim']))\n",
    "    torch.save(G.state_dict(), 'epochs/G_epoch_%d.pth' % (epoch))\n",
    "    torch.save(D.state_dict(), 'epochs/D_epoch_%d.pth' % (epoch))\n",
    "\n",
    "    results['d_loss'].append(epoch_res['d_loss'] / epoch_res['batch_size'])\n",
    "    results['g_loss'].append(epoch_res['g_loss'] / epoch_res['batch_size'])\n",
    "    results['d_score'].append(epoch_res['d_score'] / epoch_res['batch_size'])\n",
    "    results['g_score'].append(epoch_res['g_score'] / epoch_res['batch_size'])\n",
    "    results['psnr'].append(val_res['psnr'])\n",
    "    results['ssim'].append(val_res['ssim'])\n",
    "    \n",
    "    if epoch != 0:\n",
    "        out_path = 'statistics/'\n",
    "        data_frame = pd.DataFrame(\n",
    "            data={'Loss_D': results['d_loss'], 'Loss_G': results['g_loss'], 'Score_D': results['d_score'],\n",
    "                  'Score_G': results['g_score'], 'PSNR': results['psnr'], 'SSIM': results['ssim']},\n",
    "            index=range(1, epoch + 1))\n",
    "        data_frame.to_csv(out_path + 'srf_' + str(upscale) + '_train_results.csv', index_label='Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
